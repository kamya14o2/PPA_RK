---
title: 'Final: Measuring Gentrification'
author: "Kamya Khandelwal, Revathi Machan, Claudia Schreier"
date: "05/13/2024"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
    theme: journal  
---

*TO:* Akshay Malik, City of Philadelphia Smart Cities Director
*FROM:* Kamya Khandelwal, Revathi Machan, Claudia Schreier
*DATE:* 13 May 2024
*RE:* Predictive modeling for gentrification in Philadelphia

## Gentrification in Phildelphia 

Philadelphia has been experiencing a cost-of-living crisis for decades. For a city commonly referred to as the nation’s poorest largest city, there is much worry about the future of new developments and housing^1^. Philadelphia has the highest poverty rate out of the ten largest cities in the US. In 2018, around 231,000 Philadelphia households were cost-burdened, which is defined by the US Department of Housing and Urban Development as a situation when a household spends 30% or more of its income on housing costs, including rent, mortgage payments, utilities, insurance, and property taxes^2^. This problem is more severe for renters – among renters with incomes below $30,000 per year, 88% are cost-burdened and 68% are severely cost-burdened (spending more than 50% of income on housing)^3^.  

With lots of cost-burdened householders, including homeowners and renters, in the city, it  seems that the fear  of displacement and gentrification in many neighborhoods is common. Historically high rents and mortgage rates, combined with unprecedented levels of new developments have spurred discussion around gentrification in the city^4^. Gentrification is defined as a type of neighborhood change where higher income residents replace lower income ones – housing costs rise, property values rise, and typically, long term residents get displaced^5^. Gentrification today is informed by a legacy of displacement that was spurred by ”redevelopment” and ”urban renewal” plans. Philadelphia was hit especially hard by this displacement in West Philadelphia with university expansion plans throughout the second half of the 20th century.


In broader discussions about gentrification online and in media, gentrification can seem like something that occurs when a brewery replaces a corner store, or a luxury gym replaces a single-family home. Gentrification can be marked by many things, and the signs can vary for different people that a neighborhood is being gentrified.  There has even been new technology like AI models created to identify signs of gentrification from temporal maps like Google Street View^6^. While the signs of gentrification can be visible, they may be hard to track through data. Researchers rely on demographic data to make conclusions about the types of change that the neighborhood is experiencing, like racial changes, age changes, and income changes.

## Data exploration

This project seeks to create a predictive model for gentrification that can be used in planning practice to identify areas at risk for gentrification or identify areas undergoing gentrification. This model uses factors that the team at Marie Antoinette Predictions have researched and believe are reliable markers for gentrification. The variables in our model are construction permits, vacant properties, affordable housing locations, green spaces, and demolitions. The data was collected from Philadelphia’s open data portal^7^.  After downloading the relevant data, it was cleaned to ensure that variables were clear and transferable. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)


if(!require(pacman)){install.packages("pacman"); library(pacman)}
p_load(sf, tidyverse, tidycensus, RSocrata, viridis, spatstat, raster, spdep, FNN, grid, gridExtra, knitr, kableExtra, classInt, RColorBrewer, ggcorrplot)

# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

```{r - map theme - ideally we replace this later}
mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 15,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.text.x = element_text(size = 14))
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(color = "darkred", size=15, face="bold"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}
```


## Read in Data from Philadelphia

```{r read_data}
# permits
phlPermits <- 
  st_read("https://phl.carto.com/api/v2/sql?q=SELECT+*+FROM+permits+WHERE+permitissuedate+>=+'2013-01-01'+AND+permitissuedate+<+'2022-12-31'&filename=permits&format=geojson&skipfields=cartodb_id") %>% 
    st_transform('ESRI:102728')%>%
  mutate(Legend = "Philadelphia Permits")%>%
  mutate(Year = as.integer(format(permitissuedate, "%Y"))) 

## 2. Philadelphia Boundaries
phlBoundary <- 
  st_read("http://data.phl.opendata.arcgis.com/datasets/405ec3da942d4e20869d4e1449a2be48_0.geojson") %>%
  st_transform('ESRI:102728')

#dunno if we need to fishnet this
phlFishnet <- 
  st_make_grid(phlBoundary,
               cellsize = 500, 
               square = TRUE) %>%
  .[phlBoundary] %>% 
  st_sf() %>%
  mutate(uniqueID = rownames(.))

## 3. Philadelphia Neighborhoods - isn't loading
phillyNeighborhoods <-
  st_read("https://raw.githubusercontent.com/opendataphilly/open-geo-data/master/philadelphia-neighborhoods/philadelphia-neighborhoods.geojson") %>%
  st_transform('ESRI:102728') %>%
  st_transform(st_crs(phlFishnet)) 

## 4. Vacant Property
vacantBuilding <-
  st_read('https://opendata.arcgis.com/datasets/f7ed68293c5e40d58f1de9c8435c3e84_0.geojson') %>% 
  na.omit() %>%
  st_transform('ESRI:102728') %>%
  st_transform(st_crs(phlFishnet)) %>%
  mutate(Legend = "Vacant Buildings")

## 5. Affordable Housing
affordableHousing <-
  st_read('https://opendata.arcgis.com/datasets/ca8944190b604b2aae7eff17c8dd9ef5_0.geojson') %>% 
  filter(FISCAL_YEAR_COMPLETE >= "2012") %>%
  st_transform('ESRI:102728') %>%
  st_transform(st_crs(phlFishnet)) %>%
  mutate(Legend = "Affordable Housing")


## 7. Green Spaces
greenSpace <-
  st_read('https://opendata.arcgis.com/datasets/d52445160ab14380a673e5849203eb64_0.geojson') %>% 
  st_transform('ESRI:102728') %>%
  st_transform(st_crs(phlFishnet)) %>%
  mutate(Legend = "Green Spaces")


## 10. Demolition Data
buildingDemolition <-
  st_read('https://phl.carto.com/api/v2/sql?q=SELECT+*+FROM+demolitions&filename=demolitions&format=geojson&skipfields=cartodb_id') %>% 
  mutate(year = substr(start_date,1,4)) %>%
  filter(year == '2022') %>%
  st_transform('ESRI:102728') %>%
  st_transform(st_crs(phlFishnet)) %>%
  mutate(Legend = "Building Demolition") 

## 11. Census data - ACS 2021
tracts22 <- get_acs(
  geography = "tract",
  variables = c(
    "B01003_001",   # Total Population
    "B19013_001",   # Median Household Income
    "B25008_001E",  # Total Population in Housing Units
    "B25008_002",   # Owner-Occupied Units
    "B25008_003",   # Renter-Occupied Units
    "B15003_022",   # Educational Attainment: Bachelor's Degree
    "B06012_002E",  # Population Below the Poverty Level
    "B02001_002",   # Race and Ethnicity: White Alone
    "B02001_003",   # Race and Ethnicity: Black or African American Alone
    "B27011_008E",  # Population Unemployed
    "B08105F_007E"  # Population Working from Home
    
  ),
  year = 2022,
  state = "PA",
  county = "Philadelphia",
  geometry = TRUE,
  output = "wide"
)%>%
  dplyr::select(-NAME, -ends_with("M")) %>%
  rename(TotalPop = B01003_001E,                           # Total Population
         MedHHInc = B19013_001E,                           # Median Household Income
         TotalUnit = B25008_001E,                          # Total Population in Housing Units
         OwnerOccupied = B25008_002E,                      # Owner-Occupied Units
         RenterOccupied = B25008_003E,                     # Renter-Occupied Units
         BachelorsDegree = B15003_022E,                    # Educational Attainment: Bachelor's Degree
         TotalPoverty = B06012_002E,                       # Population Below the Poverty Level
         TotalUnemployment = B27011_008E,                  # Population Unemployed
         TotalWFH = B08105F_007E,                          # Population Working from Home
         RaceWhite = B02001_002E,                          # Race and Ethnicity: White Alone
         RaceBlack = B02001_003E,                          # Race and Ethnicity: Black or African American Alone
         )

### 11.1. Transform the data to ESRI:102728 projection

tracts22 <- tracts22 %>% st_transform(st_crs(phlFishnet))

### 11.2 Create new variables

tracts22 <- tracts22 %>%
  mutate(pctWhite = ifelse(TotalPop > 0, RaceWhite / TotalPop * 100,0),
         pctBlack = ifelse(TotalPop > 0, RaceBlack / TotalPop * 100,0),
         pctPoverty = ifelse(TotalPop > 0, TotalPoverty / TotalPop *100, 0),
         pctUnemploy = ifelse(TotalPop > 0, TotalUnemployment / TotalPop *100, 0),
         pctWFH = ifelse(TotalPop > 0, TotalWFH / TotalPop *100, 0),
         pctBach = ifelse(TotalPop > 0, BachelorsDegree / TotalPop *100, 0),
         pctOwnerOccupied = ifelse(TotalPop > 0, OwnerOccupied / TotalUnit *100, 0),
         pctRenterOccupied = ifelse(TotalPop > 0, RenterOccupied / TotalUnit *100, 0)
         ) %>%
  dplyr::select(-RaceWhite, -RaceBlack, -TotalPoverty ,-TotalUnemployment,-OwnerOccupied, -RenterOccupied, -TotalUnit, -TotalWFH, -BachelorsDegree, -GEOID) %>%
  st_transform(st_crs(phlFishnet)) 

### 11.3 Organize into datasets - what is this whole organization part for??

tracts21.MedHHInc <- tracts22 %>%
  dplyr::select(MedHHInc) %>%
  rename(Legend = MedHHInc)

tracts21.pctWhite <- tracts22 %>%
  dplyr::select(pctWhite)%>%
  rename(Legend = pctWhite)

tracts21.pctBlack <- tracts22 %>%
  dplyr::select(pctBlack)%>%
  rename(Legend = pctBlack)

tracts21.pctPoverty <- tracts22 %>%
  dplyr::select(pctPoverty)%>%
  rename(Legend = pctPoverty)

tracts21.pctUnemploy <- tracts22 %>%
  dplyr::select(pctUnemploy)%>%
  rename(Legend = pctUnemploy)

tracts21.pctBach <- tracts22 %>%
   dplyr::select(pctBach)%>%
   rename(Legend = pctBach)

tracts21.pctWFH <- tracts22 %>%
  dplyr::select(pctWFH)%>%
  rename(Legend = pctWFH)

tracts21.pctOwnerOccupied <- tracts22 %>%
  dplyr::select(pctOwnerOccupied)%>%
  rename(Legend = pctOwnerOccupied)

tracts21.pctRenterOccupied <- tracts22 %>%
  dplyr::select(pctRenterOccupied)%>%
  rename(Legend = pctRenterOccupied)

```

```{r police data}
#Data Cleaning!

#Categorizing the permits for construction and demolition
phlPermits <- phlPermits %>%
  mutate(newType = case_when(permittype == "BUILDING" | permittype == "BP_NEWCNST"  ~ 'CONSTRUCTION PERMIT',
  permittype == "DEMOLITION" | permittype == "BP_DEMO" ~ 'DEMOLITION PERMIT'))


cnstPermits <- phlPermits %>%
  filter(newType == 'CONSTRUCTION PERMIT')

demoPermits <- phlPermits %>%
  filter(newType == 'DEMOLITION PERMIT')




```

## visualizing point data

Here I'm plotting point data and density. These areas are indicated by denser clusters of red points (individual incidents) on the left and warmer colors on the density plot on the right, suggesting a higher concentration of assault crimes.

```{r fig.width=6, fig.height=4}
# Uses grid.arrange to organize independent plots - do we need this?

  # Plot 1: map of all construction and demo permits issued b/w 2013 and 2022
ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    geom_sf(data = phlPermits, aes(colour = newType), size = 0/5, show.legend = "point") +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Major Permits Issued, 2013-22 in Philadelphia",
       caption = "Figure 1") +
    theme_void()  
  

  # Plot 2 + 3: Mapped points and Density map of construction permits issued
ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    geom_sf(data = cnstPermits, aes(colour = "darkgreen"), size = 0.5, show.legend = "point") + #why is the color literally orange???
  labs(title = "Construction Permits Issued, 2013-22 in Philadelphia",
       caption = "Figure 2") +
    theme_void() 

#grid.arrange( - idk what the grid arrange stuff is for
 #ncol = 2,
  ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    stat_density2d(data = data.frame(st_coordinates(cnstPermits)),  
                   aes(X, Y, fill = ..level.., alpha = ..level..),  # Define aesthetics for density contours
                   size = 0.01, bins = 40, geom = 'polygon') +  # Set size and number of bins for contours
    scale_fill_viridis_c(option = "plasma") +  # Use Viridis color scale for fill
    scale_alpha(range = c(0.00, 0.35), guide = FALSE) +  # Set transparency range for contours
    labs(title = "Density of Construction Permits") +  # Set plot title
    theme_void() + theme(legend.position = "none")  # Use a blank theme and remove legend



  #Plot 4 + 5: Mapped points and Density map of demolition permits issued
ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    geom_sf(data = demoPermits, aes(colour = newType), size = 0/5, show.legend = "point") +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Demolition Permits Issued, 2013-22 in Philadelphia",
       caption = "Figure 4") +
    theme_void()

ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    stat_density2d(data = data.frame(st_coordinates(demoPermits)),  
                   aes(X, Y, fill = ..level.., alpha = ..level..),  # Define aesthetics for density contours
                   size = 0.01, bins = 40, geom = 'polygon') +  # Set size and number of bins for contours
    scale_fill_viridis_c(option = "plasma") +  # Use Viridis color scale for fill
    scale_alpha(range = c(0.00, 0.35), guide = FALSE) +  # Set transparency range for contours
    labs(title = "Density of Construction Permits") +  # Set plot title
    theme_void() + theme(legend.position = "none")  # Use a blank theme and remove legend


# plot 6: affordable housing
ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    geom_sf(data = affordableHousing, aes(colour = 'darkgreen'), size = 1, show.legend = "point") +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Affordable Housing Developments, 2012-19 in Philadelphia",
       caption = "Figure 6") +
    theme_void()


#plot7: home values

#plot8 and 9: vacant land point and density maps
ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    geom_sf(data = vacantBuilding, aes(colour = 'darkred'), size = 0/5, show.legend = "point") +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Suspected Vacant Buildings in Philadelphia",
       caption = "Figure 8") +
    theme_void()

ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    stat_density2d(data = data.frame(st_coordinates(vacantBuilding)),  
                   aes(X, Y, fill = ..level.., alpha = ..level..),  # Define aesthetics for density contours
                   size = 0.01, bins = 40, geom = 'polygon') +  # Set size and number of bins for contours
    scale_fill_viridis_c(option = "plasma") +  # Use Viridis color scale for fill
    scale_alpha(range = c(0.00, 0.35), guide = FALSE) +  # Set transparency range for contours
    labs(title = "Density of Vacant Buldings") +  # Set plot title
    theme_void() + theme(legend.position = "none")  # Use a blank theme and remove legend

#plot10: demographic
ggplot()+
    geom_sf(data=tracts22, aes(color=NA, fill=pctUnemploy))+
    scale_color_viridis()+
    scale_fill_viridis()+
    geom_sf(data = cnstPermits, aes(color="#FE9900") ,
          show.legend = "point", size = .1, alpha=0.3) +
    scale_color_identity() +
    labs(title="% Unemployment around Permits Issued")+
    mapTheme()+theme(plot.title = element_text(size = 10), legend.title=element_blank())

ggplot()+
    geom_sf(data=tracts22, aes(color=NA, fill=pctBach))+
    scale_color_viridis()+
    scale_fill_viridis()+
    geom_sf(data = cnstPermits, aes(color="#FE9900") ,
          show.legend = "point", size = .1, alpha=0.3) +
    scale_color_identity() +
    labs(title="% Population with Bachelor's Degree around Permits Issued")

```


### Aggregate points to the fishnet

```{r fishnet}



# Attaching datasets on spatial factors to Fishnet

## 1. Extracting geometry for spatial factors


affordableHousings <- affordableHousing %>%
  dplyr::select(geometry, Legend)

vacantBuildings <- vacantBuilding %>%
  dplyr::select(geometry, Legend)

greenSpaces <- greenSpace %>%
  dplyr::select(geometry, Legend)

buildingDemolitions <- buildingDemolition %>%
  dplyr::select(geometry, Legend)


## 2. Creating fishnet of spatial factor variables 

vars_net <- 
  rbind(affordableHousings, vacantBuildings,
        greenSpaces, buildingDemolitions) %>%
  st_join(., phlFishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  full_join(phlFishnet, by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>%
  st_sf() %>%
  na.omit() %>% 
  dplyr::select(-`<NA>`) %>%
  ungroup()


cnstPermits <- st_transform(cnstPermits, st_crs(phlFishnet))

construction_net <- 
  dplyr::select(cnstPermits) %>% 
  mutate(countPermits = 1) %>% 
  aggregate(., phlFishnet, sum) %>%
  mutate(countPermits = replace_na(countPermits, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(phlFishnet) / 24), 
                       size=nrow(phlFishnet), replace = TRUE))
```

## Nearest Neighbor Feature

This code calculates the nearest neighbors (NN) of alley lights out data to the centroids of fishnet grid cells. It first defines two convenience aliases st_c and st_coid for st_coordinates and st_centroid functions, respectively, to reduce the length of function names. Then, it creates a new column named lightsout.nn in the vars_net dataframe using a custom nn_function. This function finds the nearest neighbors of the centroids of fishnet grid cells to the alley lights out locations, considering the 3 closest neighbors (k = 3).

```{r knn}

## 1.2. Vacant Buildings

### Mapping nearest feature

nearest_vacantBuilding <- sf::st_nearest_feature(vars_net, vacantBuilding)

### Converting to rsgeo geometries

x <- rsgeo::as_rsgeo(vars_net)
y <- rsgeo::as_rsgeo(vacantBuilding)

### Calculating distance

vars_net$dist_vacantBuilding <- rsgeo::distance_euclidean_pairwise(x, y[nearest_vacantBuilding])


## 1.3. Affordable Housing

### Mapping nearest feature

nearest_affordableHousing <- sf::st_nearest_feature(vars_net, affordableHousing)

### Converting to rsgeo geometries

x <- rsgeo::as_rsgeo(vars_net)
y <- rsgeo::as_rsgeo(affordableHousing)

### Calculating distance

vars_net$dist_affordableHousing <- rsgeo::distance_euclidean_pairwise(x, y[nearest_affordableHousing])


## 1.4. Green Spaces

### Mapping nearest feature

nearest_greenSpace <- sf::st_nearest_feature(vars_net, greenSpace)

### Converting to rsgeo geometries

x <- rsgeo::as_rsgeo(vars_net)
y <- rsgeo::as_rsgeo(greenSpace)

### Calculating distance

vars_net$dist_greenSpace <- rsgeo::distance_euclidean_pairwise(x, y[nearest_greenSpace])

## 1.8. Building Demolitions

### Mapping nearest feature

nearest_buildingDemolition <- sf::st_nearest_feature(vars_net, buildingDemolition)

### Converting to rsgeo geometries

x <- rsgeo::as_rsgeo(vars_net)
y <- rsgeo::as_rsgeo(buildingDemolition)

### Calculating distance

vars_net$dist_buildingDemolition <- rsgeo::distance_euclidean_pairwise(x, y[nearest_buildingDemolition])


```


```{r vizNN}
# 2. Visualizing nearest distance for spatial factors on Fishnet

## 2.1. Visualizing the nearest three features

vars_net.long.nn <- 
  dplyr::select(vars_net, starts_with("dist")) %>%
  gather(Variable, value, -geometry)

vars <- unique(vars_net.long.nn$Variable)
mapList <- list()

for(i in vars){
  mapList[[i]] <- 
    ggplot() +
    geom_sf(data = filter(vars_net.long.nn, Variable == i), aes(fill=value), colour=NA) +
    scale_fill_viridis_c(option = "plasma",
                         name = " ") +
    labs(title=i) +
    mapTheme()+
    theme(plot.title = element_text(size = 12, color = "black"))
  }

bottomCaption <- textGrob("Figure 8", gp = gpar(hjust = 0))

do.call(grid.arrange, c(list(grobs = mapList, ncol = 2, 
                             top = textGrob("Spatial Factors: Nearest Neighbor Distance for Permits Issued\n", 
                                            gp = gpar(fontsize = 15, fontface = "bold", col = "darkred")), 
                             bottom = bottomCaption)))

```

## Join NN feature to our fishnet

Since the counts were aggregated to each cell by `uniqueID` we can use that to join the counts to the fishnet.

```{r join census data to fishnet - not sure why this is necessary}
# Joining Census Data to Fishnet

tracts22 <- tracts22 %>%
  filter(TotalPop>0)

vars_net <-
  vars_net%>%
  st_centroid()%>%
  st_join(tracts22)

library(dplyr)
vars_net <- vars_net %>% mutate_all(~replace(., is.na(.), 0))

# Perform Spatial Join of variables with permits

final_net <-
  left_join(construction_net, st_drop_geometry(vars_net), by="uniqueID") # this one doesn't work so the last one won't either

# Final Net


final_net <-
  st_centroid(final_net) %>% 
    st_join(dplyr::select(phillyNeighborhoods, name), by = "uniqueID") %>% 
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%  
      st_sf() %>%
  na.omit()
```



## Local Moran's I for fishnet grid cells

Using {spdep} package to to build neighborhood weights and list to calculate local Moran's I.

```{r}
## generates warnings from PROJ issues
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weigths
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

# print(final_net.weights, zero.policy=TRUE)
```

```{r local moran}
## see ?localmoran
local_morans <- localmoran(final_net$countPermits, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(Permit_Count = countPermits, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse((P_Value <= 0.05), 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```

### Plotting local Moran's I results

This is a complex code chunk - it's a loop which builds ggplots of local Moran's for each of your `vars`.
It is important to recognize that we can get high local moran's I values in the case of either high values near other high values or low values near other low values. If you check out the equation for LMI, you'll see that it is driven by a calulation for all pairs of neighbors, that looks at the (in this case) home values of one neighborhood minus the mean of all home values in the study, times home values of an adjacent neighborhood minus the mean. Therefore, since we multiply them together, two values below the mean will yield and positive value as well two values above the mean. In the code below, we examine the local moran's I value, the p-value, and extracts hotspots (that can be hot or cold!) based on the p-value.

The maps show areas where high values cluster together (hotspots), areas of low value clustering (cold spots), and the significance of these clusters, indicating where these patterns are statistically meaningful.The significant hotspots in the South may be due to a combination of socio-economic factors, such as poverty, unemployment, and urban decay, which are historically linked to higher crime rates. Additionally, this area has been noted for gang activity which contributes to violence.

```{r fig.width=10, fig.height=4}
## This is just for plotting
vars <- unique(final_net.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      theme_void() + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I Statistics, Construction Permits"))
```

Now, we need to actually find the clusters of high-high values that exist in the upper-right quadrant of a local moran's I plot. This plots the original values (lightsout) and the spatial lag of that value (wx), the average lights out value for each cells' neighboring cells. Not the scale function in front of lihgts out in the mp function. This places the values on a z-score so that the average value is 0 and positive values are above the mean (1= 1sd above the mean). A 'high' value technically means it is above the mean of the city. This code then looks for values of the cell that are above the mean and values of its neighborhs that are above the mean and only takes those that are statistically significant. We create a binary variable called 'hotspot' that meets these criteria.

The chart suggests a positive spatial autocorrelation for alley lights out incidents, as indicated by the clustering of data points along the line of fit. 

```{r lmi_hotspot}

#lmoran <- localmoran(final_net$lightsout, final_net.weights,  zero.policy=TRUE)

#final_net$lmI <- lmoran[, "Ii"] # local Moran's I
#final_net$lmZ <- lmoran[, "Z.Ii"] # z-scores
#final_net$lmp <- lmoran[, "Pr(z != E(Ii))"]


#mp <- moran.plot(as.vector(scale(final_net$lightsout)), final_net.weights, zero.policy = TRUE)

##Create a hotspot variable:
#final_net$lmp <- ifelse(is.nan(final_net$lmp), 0.10, final_net$lmp)
#final_net$hotspot <- 0
#final_net[(mp$x >=0 & mp$wx >=0) & final_net$lmp <= 0.05, "hotspot"]<- 1
```

Now we will calculate distance to that nearest hotspot.


```{r}
# generates warning from NN
#final_net <- final_net %>% 
  #mutate(lightsout.isSig.dist = 
           #nn_function(st_c(st_coid(final_net)),
                       #st_c(st_coid(filter(final_net, 
                                      #     hotspot == 1))), 
#                       k = 1))

```

### Plot NN distance to hot spot

Lighter areas represent locations with the longest distances to the nearest reported alley light outage, suggesting lower spatial density of such reports.

```{r}
#ggplot() +
#      geom_sf(data = final_net, aes(fill=lightsout.isSig.dist), colour=NA) +
#      scale_fill_viridis(name="NN Distance") +
#      labs(title="Alley Lights Out NN Distance") +
#      theme_void()
```

## Modeling and CV

```{r - correlation tests for spatial and demographic factors}

correlation.long <-
  st_drop_geometry(final_net) %>%
    dplyr::select(-uniqueID, -cvID, -name) %>%
    gather(Variable, Value, -countPermits) %>%
  mutate(Value = as.numeric(Value))

correlation.cor <-
  correlation.long %>%
    group_by(Variable) %>%
    summarize(correlation = cor(Value, countPermits, use = "complete.obs"))

# Visualizing correlations through scatter plots
#we have a lot of demographic variables here that i don't know if we necessarily need or are interested in keeping for our final stuff - think it may be better to pick and choose fewer demo variables and maybe choose more external variables
    
ggplot(correlation.long, aes(Value, countPermits)) +
  geom_point(size = 0.1) +
  geom_text(data = correlation.cor, aes(label = paste("r =", round(correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +
  geom_smooth(method = "lm", se = FALSE, colour = "orange") +
  facet_wrap(~Variable, ncol = 4, scales = "free") +
  labs(title = "Permit Count as a function of risk factors", caption="Figure 12") +
  plotTheme()

```


```{r - correlation matrix}

numvars <- c("countPermits", "dist_vacantBuilding",  "dist_affordableHousing","dist_greenSpace", "dist_buildingDemolition", "TotalPop", "MedHHInc", "pctWhite", "pctBlack", "pctBach" ,"pctPoverty", "pctUnemploy", "pctWFH", "pctOwnerOccupied", "pctRenterOccupied")

numeric <- final_net %>%
  st_drop_geometry(final_net) %>%
  dplyr::select(numvars)%>%
  na.omit()

ggcorrplot(
  round(cor(numeric), 1), 
  p.mat = cor_pmat(numeric),
  colors = c('#d7191c','white','#2c7bb6'),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across Variables\n", caption="Figure 13")+ 
    theme(plot.title = element_text(size = 11, face = "bold", color = "darkred"))+
    theme(axis.text.x=element_text(size=8))+
    theme(axis.text.y=element_text(size=8))

```






##Regression

```{r results='hide'}

#View(crossValidate)

## define the variables we want
reg.ss.vars <- c("countPermits", "dist_vacantBuilding",  "dist_affordableHousing","dist_greenSpace", "dist_buildingDemolition", "TotalPop", "MedHHInc", "pctWhite", "pctBlack", "pctBach" ,"pctPoverty", "pctUnemploy", "pctWFH", "pctOwnerOccupied", "pctRenterOccupied")



## RUN REGRESSIONS - neither are working
final_net$name <- ifelse(is.na(final_net$name), "UNKNOWN", final_net$name)

reg.ss.cv <- crossValidate(
  dataset = final_net,
  id = "cvID",
  dependentVariable = "countPermits",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = -cvID, -countPermits, Prediction, -geometry)


reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "name",                        
  dependentVariable = "countPermits",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = name, countPermits, Prediction, geometry)
```

```{r}
# calculate errors by NEIGHBORHOOD
error_by_reg_and_fold <- 
  reg.ss.spatialCV %>% #this reg doesn't work to run the rest of the code
    group_by(cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countPermit, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

error_by_reg_and_fold %>% 
  arrange(desc(MAE))
error_by_reg_and_fold %>% 
  arrange(MAE)

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
  scale_x_continuous(breaks = seq(0, 11, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "LOGO-CV",
         x="Mean Absolute Error", y="Count") 
```
It seems that the prediction model's accuracy varies across different neighborhoods, with some areas showing larger errors than others. 

## Density vs predictions

The `spatstat` function gets us kernal density estimates with varying search radii.

```{r kd}
# demo of kernel width
permits_ppp <- as.ppp(st_coordinates(cnstPermits), W = st_bbox(final_net))
permits_KD.1000 <- density.ppp(permits_ppp, 1000)
permits_KD.1500 <- density.ppp(permits_ppp, 1500)
permits_KD.2000 <-density.ppp(permits_ppp, 2000)
permits_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(permits_KD.1000), as(phillyNeighborhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(permits_KD.1500), as(phillyNeighborhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(permits_KD.2000), as(phillyNeighborhoods, 'Spatial')))), Legend = "2000 Ft.")) 

permits_KD.df$Legend <- factor(permits_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=permits_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel density with 3 different search radii") +
  theme_void()
```
Areas with higher concentrations of assault incidents are indicated by warmer colors, showing where such crimes were most prevalent in the city.

```{r kd2}

#works but i dont think is needed unless there's one specific tihng we wanna look at 

as.data.frame(permits_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
     geom_sf(aes(fill=value)) +
     geom_sf(data = sample_n(cnstPermits, 1500), size = .5) +
     scale_fill_viridis(name = "Density") +
     labs(title = "Kernel density of Construction Permits") +
     theme_void()
```

## Getting Chicago city data

Let's see how our model performed relative to KD on the following year's data.

```{r 2018 data}
assault18 <- 
  read.socrata("https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy") %>% 
  filter(Primary.Type == "ASSAULT") %>%
  mutate(x = gsub("[()]", "", Location)) %>%
  separate(x,into= c("Y","X"), sep=",") %>%
  mutate(X = as.numeric(X),
         Y = as.numeric(Y)) %>% 
  na.omit %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102271') %>% 
  distinct() %>%
  .[fishnet,]
```

```{r new}

as_KDE_sum <- as.data.frame(as_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) 
kde_breaks <- classIntervals(as_KDE_sum$value, 
                             n = 5, "fisher")
as_KDE_sf <- as_KDE_sum %>%
  mutate(label = "Kernel Density",
         Risk_Category = classInt::findCols(kde_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(assault18) %>% mutate(asCount = 1), ., sum) %>%
    mutate(asCount = replace_na(asCount, 0))) %>%
  dplyr::select(label, Risk_Category, asCount)
```

```{r comp}
ml_breaks <- classIntervals(reg.ss.spatialCV$Prediction, 
                             n = 5, "fisher")
as_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category =classInt::findCols(ml_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(assault18) %>% mutate(asCount = 1), ., sum) %>%
      mutate(asCount = replace_na(asCount, 0))) %>%
  dplyr::select(label,Risk_Category, asCount)
```

We don't do quite as well because we don't have very many features, but still pretty good.

```{r comp plot}
rbind(as_KDE_sf, as_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(assault18, 3000), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2017 assault risk predictions; 2018 assault") +
    mapTheme(title_size = 14)
```
The maps show where assaults happened in 2017 and where they might happen in 2018. This shows us that we can use old data to guess where crimes might happen next.

```{r comp bar}
rbind(as_KDE_sf, as_risk_sf) %>%
  st_drop_geometry() %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countAssault = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Pcnt_of_test_set_crimes = countAssault / sum(countAssault)) %>%
    ggplot(aes(Risk_Category,Pcnt_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE, name = "Model") +
      labs(title = "Risk prediction vs. Kernel density, 2018 assaults",
           y = "% of Test Set Assaults (per model)",
           x = "Risk Category") +
  theme_bw() +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```

## Conclusion

## References

1. https://whyy.org/articles/philadelphia-americas-poorest-big-city-poverty/
2. https://www.pewtrusts.org/-/media/assets/2020/09/phillyhousingreport.pdf
3. Ibid.
4. https://whyy.org/articles/philadelphia-affordable-housing-bills-council-gauthier/
5. https://sites.utexas.edu/gentrificationproject/understanding-gentrification-and-displacement/
6. https://hai.stanford.edu/news/spotting-visual-signs-gentrification-scale
7. https://opendataphilly.org/