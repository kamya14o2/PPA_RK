---
title: 'Final: Measuring Gentrification'
author: "Kamya Khandelwal, Revathi Machan, Claudia Schreier"
date: "05/13/2024"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
    theme: journal  
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)


if(!require(pacman)){install.packages("pacman"); library(pacman)}
p_load(sf, tidyverse, tidycensus, RSocrata, viridis, spatstat, raster, spdep, FNN, grid, gridExtra, knitr, kableExtra, classInt, RColorBrewer)

# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

```{r - map theme - ideally we replace this later}
mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 15,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.text.x = element_text(size = 14))
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(color = "darkred", size=15, face="bold"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}
```


## Read in Data from Philadelphia

```{r read_data}
# permits
phlPermits <- 
  st_read("https://phl.carto.com/api/v2/sql?q=SELECT+*+FROM+permits+WHERE+permitissuedate+>=+'2013-01-01'+AND+permitissuedate+<+'2022-12-31'&filename=permits&format=geojson&skipfields=cartodb_id") %>% 
    st_transform('ESRI:102728')%>%
  mutate(Legend = "Philadelphia Permits")%>%
  mutate(Year = as.integer(format(permitissuedate, "%Y"))) 

## 2. Philadelphia Boundaries
phlBoundary <- 
  st_read("http://data.phl.opendata.arcgis.com/datasets/405ec3da942d4e20869d4e1449a2be48_0.geojson") %>%
  st_transform('ESRI:102728')

#dunno if we need to fishnet this
phlFishnet <- 
  st_make_grid(phlBoundary,
               cellsize = 500, 
               square = TRUE) %>%
  .[phlBoundary] %>% 
  st_sf() %>%
  mutate(uniqueID = rownames(.))

## 3. Philadelphia Neighborhoods - isn't loading
#phillyNeighborhoods <-
  #st_read("https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson") %>%
  #st_transform('ESRI:102728') %>%
  #st_transform(st_crs(phlFishnet)) 

## 4. Vacant Property
vacantBuilding <-
  st_read('https://opendata.arcgis.com/datasets/f7ed68293c5e40d58f1de9c8435c3e84_0.geojson') %>% 
  na.omit() %>%
  st_transform('ESRI:102728') %>%
  st_transform(st_crs(phlFishnet)) %>%
  mutate(Legend = "Vacant Buildings")

## 5. Affordable Housing
affordableHousing <-
  st_read('https://opendata.arcgis.com/datasets/ca8944190b604b2aae7eff17c8dd9ef5_0.geojson') %>% 
  filter(FISCAL_YEAR_COMPLETE >= "2012") %>%
  st_transform('ESRI:102728') %>%
  st_transform(st_crs(phlFishnet)) %>%
  mutate(Legend = "Affordable Housing")


## 7. Green Spaces
greenSpace <-
  st_read('https://opendata.arcgis.com/datasets/d52445160ab14380a673e5849203eb64_0.geojson') %>% 
  st_transform('ESRI:102728') %>%
  st_transform(st_crs(phlFishnet)) %>%
  mutate(Legend = "Green Spaces")


## 10. Demolition Data
buildingDemolition <-
  st_read('https://phl.carto.com/api/v2/sql?q=SELECT+*+FROM+demolitions&filename=demolitions&format=geojson&skipfields=cartodb_id') %>% 
  mutate(year = substr(start_date,1,4)) %>%
  filter(year == '2022') %>%
  st_transform('ESRI:102728') %>%
  st_transform(st_crs(phlFishnet)) %>%
  mutate(Legend = "Building Demolition") 

## 11. Census data - ACS 2021
tracts22 <- get_acs(
  geography = "tract",
  variables = c(
    "B01003_001",   # Total Population
    "B19013_001",   # Median Household Income
    "B25008_001E",  # Total Population in Housing Units
    "B25008_002",   # Owner-Occupied Units
    "B25008_003",   # Renter-Occupied Units
    "B15003_022",   # Educational Attainment: Bachelor's Degree
    "B06012_002E",  # Population Below the Poverty Level
    "B02001_002",   # Race and Ethnicity: White Alone
    "B02001_003",   # Race and Ethnicity: Black or African American Alone
    "B27011_008E",  # Population Unemployed
    "B08105F_007E"  # Population Working from Home
    
  ),
  year = 2022,
  state = "PA",
  county = "Philadelphia",
  geometry = TRUE,
  output = "wide"
)%>%
  dplyr::select(-NAME, -ends_with("M")) %>%
  rename(TotalPop = B01003_001E,                           # Total Population
         MedHHInc = B19013_001E,                           # Median Household Income
         TotalUnit = B25008_001E,                          # Total Population in Housing Units
         OwnerOccupied = B25008_002E,                      # Owner-Occupied Units
         RenterOccupied = B25008_003E,                     # Renter-Occupied Units
         BachelorsDegree = B15003_022E,                    # Educational Attainment: Bachelor's Degree
         TotalPoverty = B06012_002E,                       # Population Below the Poverty Level
         TotalUnemployment = B27011_008E,                  # Population Unemployed
         TotalWFH = B08105F_007E,                          # Population Working from Home
         RaceWhite = B02001_002E,                          # Race and Ethnicity: White Alone
         RaceBlack = B02001_003E,                          # Race and Ethnicity: Black or African American Alone
         )

### 11.1. Transform the data to ESRI:102728 projection

tracts22 <- tracts22 %>% st_transform(st_crs(phlFishnet))

### 11.2 Create new variables

tracts22 <- tracts22 %>%
  mutate(pctWhite = ifelse(TotalPop > 0, RaceWhite / TotalPop * 100,0),
         pctBlack = ifelse(TotalPop > 0, RaceBlack / TotalPop * 100,0),
         pctPoverty = ifelse(TotalPop > 0, TotalPoverty / TotalPop *100, 0),
         pctUnemploy = ifelse(TotalPop > 0, TotalUnemployment / TotalPop *100, 0),
         pctWFH = ifelse(TotalPop > 0, TotalWFH / TotalPop *100, 0),
         pctBach = ifelse(TotalPop > 0, BachelorsDegree / TotalPop *100, 0),
         pctOwnerOccupied = ifelse(TotalPop > 0, OwnerOccupied / TotalUnit *100, 0),
         pctRenterOccupied = ifelse(TotalPop > 0, RenterOccupied / TotalUnit *100, 0)
         ) %>%
  dplyr::select(-RaceWhite, -RaceBlack, -TotalPoverty ,-TotalUnemployment,-OwnerOccupied, -RenterOccupied, -TotalUnit, -TotalWFH, -BachelorsDegree, -GEOID) %>%
  st_transform(st_crs(phlFishnet)) 

### 11.3 Organize into datasets - what is this whole organization part for??

tracts21.MedHHInc <- tracts22 %>%
  dplyr::select(MedHHInc) %>%
  rename(Legend = MedHHInc)

tracts21.pctWhite <- tracts22 %>%
  dplyr::select(pctWhite)%>%
  rename(Legend = pctWhite)

tracts21.pctBlack <- tracts22 %>%
  dplyr::select(pctBlack)%>%
  rename(Legend = pctBlack)

tracts21.pctPoverty <- tracts22 %>%
  dplyr::select(pctPoverty)%>%
  rename(Legend = pctPoverty)

tracts21.pctUnemploy <- tracts22 %>%
  dplyr::select(pctUnemploy)%>%
  rename(Legend = pctUnemploy)

tracts21.pctBach <- tracts22 %>%
   dplyr::select(pctBach)%>%
   rename(Legend = pctBach)

tracts21.pctWFH <- tracts22 %>%
  dplyr::select(pctWFH)%>%
  rename(Legend = pctWFH)

tracts21.pctOwnerOccupied <- tracts22 %>%
  dplyr::select(pctOwnerOccupied)%>%
  rename(Legend = pctOwnerOccupied)

tracts21.pctRenterOccupied <- tracts22 %>%
  dplyr::select(pctRenterOccupied)%>%
  rename(Legend = pctRenterOccupied)

```

```{r police data}
#Data Cleaning!

#Categorizing the permits for construction and demolition
phlPermits <- phlPermits %>%
  mutate(newType = case_when(permittype == "BUILDING" | permittype == "BP_NEWCNST"  ~ 'CONSTRUCTION PERMIT',
  permittype == "DEMOLITION" | permittype == "BP_DEMO" ~ 'DEMOLITION PERMIT'))


cnstPermits <- phlPermits %>%
  filter(newType == 'CONSTRUCTION PERMIT')

demoPermits <- phlPermits %>%
  filter(newType == 'DEMOLITION PERMIT')




```

## visualizing point data

Here I'm plotting point data and density. These areas are indicated by denser clusters of red points (individual incidents) on the left and warmer colors on the density plot on the right, suggesting a higher concentration of assault crimes.

```{r fig.width=6, fig.height=4}
# Uses grid.arrange to organize independent plots - do we need this?

  # Plot 1: map of all construction and demo permits issued b/w 2013 and 2022
ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    geom_sf(data = phlPermits, aes(colour = newType), size = 0/5, show.legend = "point") +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Major Permits Issued, 2013-22 in Philadelphia",
       caption = "Figure 1") +
    theme_void()  
  

  # Plot 2 + 3: Mapped points and Density map of construction permits issued
ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    geom_sf(data = cnstPermits, aes(colour = "darkgreen"), size = 0.5, show.legend = "point") + #why is the color literally orange???
  labs(title = "Construction Permits Issued, 2013-22 in Philadelphia",
       caption = "Figure 2") +
    theme_void() 

#grid.arrange( - idk what the grid arrange stuff is for
 #ncol = 2,
  ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    stat_density2d(data = data.frame(st_coordinates(cnstPermits)),  
                   aes(X, Y, fill = ..level.., alpha = ..level..),  # Define aesthetics for density contours
                   size = 0.01, bins = 40, geom = 'polygon') +  # Set size and number of bins for contours
    scale_fill_viridis_c(option = "plasma") +  # Use Viridis color scale for fill
    scale_alpha(range = c(0.00, 0.35), guide = FALSE) +  # Set transparency range for contours
    labs(title = "Density of Construction Permits") +  # Set plot title
    theme_void() + theme(legend.position = "none")  # Use a blank theme and remove legend



  #Plot 4 + 5: Mapped points and Density map of demolition permits issued
ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    geom_sf(data = demoPermits, aes(colour = newType), size = 0/5, show.legend = "point") +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Demolition Permits Issued, 2013-22 in Philadelphia",
       caption = "Figure 4") +
    theme_void()

ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    stat_density2d(data = data.frame(st_coordinates(demoPermits)),  
                   aes(X, Y, fill = ..level.., alpha = ..level..),  # Define aesthetics for density contours
                   size = 0.01, bins = 40, geom = 'polygon') +  # Set size and number of bins for contours
    scale_fill_viridis_c(option = "plasma") +  # Use Viridis color scale for fill
    scale_alpha(range = c(0.00, 0.35), guide = FALSE) +  # Set transparency range for contours
    labs(title = "Density of Construction Permits") +  # Set plot title
    theme_void() + theme(legend.position = "none")  # Use a blank theme and remove legend


# plot 6: affordable housing
ggplot() + 
    geom_sf(data = phlBoundary, fill = "grey89", color = "darkgrey") +  
    geom_sf(data = affordableHousing, aes(colour = 'darkgreen'), size = 1, show.legend = "point") +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Affordable Housing Developments, 2012-19 in Philadelphia",
       caption = "Figure 6") +
    theme_void()


#plot7: home values

#plot8: vacant land 

#plot9: vacant land

#plot10: demographic
ggplot()+
    geom_sf(data=tracts22, aes(color=NA, fill=pctUnemploy))+
    scale_color_viridis()+
    scale_fill_viridis()+
    geom_sf(data = cnstPermits, aes(color="#FE9900") ,
          show.legend = "point", size = .1, alpha=0.3) +
    scale_color_identity() +
    labs(title="% Unemployment around Permits Issued")+
    mapTheme()+theme(plot.title = element_text(size = 10), legend.title=element_blank())

ggplot()+
    geom_sf(data=tracts22, aes(color=NA, fill=pctBach))+
    scale_color_viridis()+
    scale_fill_viridis()+
    geom_sf(data = cnstPermits, aes(color="#FE9900") ,
          show.legend = "point", size = .1, alpha=0.3) +
    scale_color_identity() +
    labs(title="% Population with Bachelor's Degree around Permits Issued")

```
The maps seem to depict areas with higher incidences of assault, which could correspond to regions known for higher levels of crime like Homan Square, the South Side of Chicago and South Shore. The chances of becoming a victim of any type of crime in South Shore is 1 in 13, according to PropertyClub. Robbery and assault are common in these neighborhoods, and murder and gun violence are much higher than the national average. 

### Aggregate points to the fishnet

```{r fishnet}



# Attaching datasets on spatial factors to Fishnet

## 1. Extracting geometry for spatial factors


affordableHousings <- affordableHousing %>%
  dplyr::select(geometry, Legend)

vacantBuildings <- vacantBuilding %>%
  dplyr::select(geometry, Legend)

greenSpaces <- greenSpace %>%
  dplyr::select(geometry, Legend)

buildingDemolitions <- buildingDemolition %>%
  dplyr::select(geometry, Legend)


## 2. Creating fishnet of spatial factor variables 

vars_net <- 
  rbind(affordableHousings, vacantBuildings,
        greenSpaces, buildingDemolitions) %>%
  st_join(., phlFishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  full_join(phlFishnet, by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>%
  st_sf() %>%
  na.omit() %>% 
  dplyr::select(-`<NA>`) %>%
  ungroup()


cnstPermits <- st_transform(cnstPermits, st_crs(phlFishnet))

construction_net <- 
  dplyr::select(cnstPermits) %>% 
  mutate(countPermits = 1) %>% 
  aggregate(., phlFishnet, sum) %>%
  mutate(countPermits = replace_na(countPermits, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(phlFishnet) / 24), 
                       size=nrow(phlFishnet), replace = TRUE))
```

## Nearest Neighbor Feature

This code calculates the nearest neighbors (NN) of alley lights out data to the centroids of fishnet grid cells. It first defines two convenience aliases st_c and st_coid for st_coordinates and st_centroid functions, respectively, to reduce the length of function names. Then, it creates a new column named lightsout.nn in the vars_net dataframe using a custom nn_function. This function finds the nearest neighbors of the centroids of fishnet grid cells to the alley lights out locations, considering the 3 closest neighbors (k = 3).

```{r knn}

## 1.2. Vacant Buildings

### Mapping nearest feature

nearest_vacantBuilding <- sf::st_nearest_feature(vars_net, vacantBuilding)

### Converting to rsgeo geometries

x <- rsgeo::as_rsgeo(vars_net)
y <- rsgeo::as_rsgeo(vacantBuilding)

### Calculating distance

vars_net$dist_vacantBuilding <- rsgeo::distance_euclidean_pairwise(x, y[nearest_vacantBuilding])


## 1.3. Affordable Housing

### Mapping nearest feature

nearest_affordableHousing <- sf::st_nearest_feature(vars_net, affordableHousing)

### Converting to rsgeo geometries

x <- rsgeo::as_rsgeo(vars_net)
y <- rsgeo::as_rsgeo(affordableHousing)

### Calculating distance

vars_net$dist_affordableHousing <- rsgeo::distance_euclidean_pairwise(x, y[nearest_affordableHousing])


## 1.4. Green Spaces

### Mapping nearest feature

nearest_greenSpace <- sf::st_nearest_feature(vars_net, greenSpace)

### Converting to rsgeo geometries

x <- rsgeo::as_rsgeo(vars_net)
y <- rsgeo::as_rsgeo(greenSpace)

### Calculating distance

vars_net$dist_greenSpace <- rsgeo::distance_euclidean_pairwise(x, y[nearest_greenSpace])

## 1.8. Building Demolitions

### Mapping nearest feature

nearest_buildingDemolition <- sf::st_nearest_feature(vars_net, buildingDemolition)

### Converting to rsgeo geometries

x <- rsgeo::as_rsgeo(vars_net)
y <- rsgeo::as_rsgeo(buildingDemolition)

### Calculating distance

vars_net$dist_buildingDemolition <- rsgeo::distance_euclidean_pairwise(x, y[nearest_buildingDemolition])


```


```{r vizNN}
# 2. Visualizing nearest distance for spatial factors on Fishnet

## 2.1. Visualizing the nearest three features

vars_net.long.nn <- 
  dplyr::select(vars_net, starts_with("dist")) %>%
  gather(Variable, value, -geometry)

vars <- unique(vars_net.long.nn$Variable)
mapList <- list()

for(i in vars){
  mapList[[i]] <- 
    ggplot() +
    geom_sf(data = filter(vars_net.long.nn, Variable == i), aes(fill=value), colour=NA) +
    scale_fill_viridis_c(option = "plasma",
                         name = " ") +
    labs(title=i) +
    mapTheme()+
    theme(plot.title = element_text(size = 12, color = "black"))
  }

bottomCaption <- textGrob("Figure 8", gp = gpar(hjust = 0))

do.call(grid.arrange, c(list(grobs = mapList, ncol = 2, 
                             top = textGrob("Spatial Factors: Nearest Neighbor Distance for Permits Issued\n", 
                                            gp = gpar(fontsize = 15, fontface = "bold", col = "darkred")), 
                             bottom = bottomCaption)))

```

## Join NN feature to our fishnet

Since the counts were aggregated to each cell by `uniqueID` we can use that to join the counts to the fishnet.

```{r join census data to fishnet - not sure why this is necessary}
# Joining Census Data to Fishnet

tracts22 <- tracts22 %>%
  filter(TotalPop>0)

vars_net <-
  vars_net%>%
  st_centroid()%>%
  st_join(tracts22)

library(dplyr)
vars_net <- vars_net %>% mutate_all(~replace(., is.na(.), 0))

# Perform Spatial Join of variables with permits

final_net <-
  left_join(construction_net, st_drop_geometry(vars_net), by="uniqueID") # this one doesn't work so the last one won't either

# Final Net

final_net <-
  st_centroid(final_net) %>% 
      st_join(dplyr::select(final_net, geometry, uniqueID)) %>%  
      st_sf() %>%
  na.omit()

```

### Join in areal data

Using spatial joins to join *centroids* of fishnets to polygon for neighborhoods and districts.

```{r areal data}

final_net <-
  st_centroid(final_net) %>%
    st_join(dplyr::select(neighborhoods, name), by = "uniqueID") %>%
    st_join(dplyr::select(policeDistricts, District), by = "uniqueID") %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

# for live demo
# mapview::mapview(final_net, zcol = "District")
```

## Local Moran's I for fishnet grid cells

Using {spdep} package to to build neighborhood weights and list to calculate local Moran's I.

```{r}
## generates warnings from PROJ issues
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weigths
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

# print(final_net.weights, zero.policy=TRUE)
```

```{r local moran}
## see ?localmoran
local_morans <- localmoran(final_net$lightsout, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(lightsout_count = lightsout.nn, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse((P_Value <= 0.05), 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```

### Plotting local Moran's I results

This is a complex code chunk - it's a loop which builds ggplots of local Moran's for each of your `vars`.
It is important to recognize that we can get high local moran's I values in the case of either high values near other high values or low values near other low values. If you check out the equation for LMI, you'll see that it is driven by a calulation for all pairs of neighbors, that looks at the (in this case) home values of one neighborhood minus the mean of all home values in the study, times home values of an adjacent neighborhood minus the mean. Therefore, since we multiply them together, two values below the mean will yield and positive value as well two values above the mean. In the code below, we examine the local moran's I value, the p-value, and extracts hotspots (that can be hot or cold!) based on the p-value.

The maps show areas where high values cluster together (hotspots), areas of low value clustering (cold spots), and the significance of these clusters, indicating where these patterns are statistically meaningful.The significant hotspots in the South may be due to a combination of socio-economic factors, such as poverty, unemployment, and urban decay, which are historically linked to higher crime rates. Additionally, this area has been noted for gang activity which contributes to violence.

```{r fig.width=10, fig.height=4}
## This is just for plotting
vars <- unique(final_net.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      theme_void() + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Assault"))
```

Now, we need to actually find the clusters of high-high values that exist in the upper-right quadrant of a local moran's I plot. This plots the original values (lightsout) and the spatial lag of that value (wx), the average lights out value for each cells' neighboring cells. Not the scale function in front of lihgts out in the mp function. This places the values on a z-score so that the average value is 0 and positive values are above the mean (1= 1sd above the mean). A 'high' value technically means it is above the mean of the city. This code then looks for values of the cell that are above the mean and values of its neighborhs that are above the mean and only takes those that are statistically significant. We create a binary variable called 'hotspot' that meets these criteria.

The chart suggests a positive spatial autocorrelation for alley lights out incidents, as indicated by the clustering of data points along the line of fit. 

```{r lmi_hotspot}

lmoran <- localmoran(final_net$lightsout, final_net.weights,  zero.policy=TRUE)

final_net$lmI <- lmoran[, "Ii"] # local Moran's I
final_net$lmZ <- lmoran[, "Z.Ii"] # z-scores
final_net$lmp <- lmoran[, "Pr(z != E(Ii))"]


mp <- moran.plot(as.vector(scale(final_net$lightsout)), final_net.weights, zero.policy = TRUE)

##Create a hotspot variable:
final_net$lmp <- ifelse(is.nan(final_net$lmp), 0.10, final_net$lmp)
final_net$hotspot <- 0
final_net[(mp$x >=0 & mp$wx >=0) & final_net$lmp <= 0.05, "hotspot"]<- 1
```

Now we will calculate distance to that nearest hotspot.


```{r}
# generates warning from NN
final_net <- final_net %>% 
  mutate(lightsout.isSig.dist = 
           nn_function(st_c(st_coid(final_net)),
                       st_c(st_coid(filter(final_net, 
                                           hotspot == 1))), 
                       k = 1))

```

### Plot NN distance to hot spot

Lighter areas represent locations with the longest distances to the nearest reported alley light outage, suggesting lower spatial density of such reports.

```{r}
ggplot() +
      geom_sf(data = final_net, aes(fill=lightsout.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Alley Lights Out NN Distance") +
      theme_void()
```

## Modeling and CV

```{r results='hide'}

# View(crossValidate)

## define the variables we want
reg.ss.vars <- c("lightsout.nn", "lightsout.isSig.dist")

## RUN REGRESSIONS
reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "name",                           
  dependentVariable = "countAssault",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = name, countAssault, Prediction, geometry)
```

```{r}
# calculate errors by NEIGHBORHOOD
error_by_reg_and_fold <- 
  reg.ss.spatialCV %>%
    group_by(cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countAssault, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

error_by_reg_and_fold %>% 
  arrange(desc(MAE))
error_by_reg_and_fold %>% 
  arrange(MAE)

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
  scale_x_continuous(breaks = seq(0, 11, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "LOGO-CV",
         x="Mean Absolute Error", y="Count") 
```
It seems that the prediction model's accuracy varies across different neighborhoods, with some areas showing larger errors than others. 

## Density vs predictions

The `spatstat` function gets us kernal density estimates with varying search radii.

```{r kd}
# demo of kernel width
as_ppp <- as.ppp(st_coordinates(assault), W = st_bbox(final_net))
as_KD.1000 <- density.ppp(as_ppp, 1000)
as_KD.1500 <- density.ppp(as_ppp, 1500)
as_KD.2000 <-density.ppp(as_ppp, 2000)
as_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(as_KD.1000), as(neighborhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(as_KD.1500), as(neighborhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(as_KD.2000), as(neighborhoods, 'Spatial')))), Legend = "2000 Ft.")) 

as_KD.df$Legend <- factor(as_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=as_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel density with 3 different search radii") +
  theme_void()
```
Areas with higher concentrations of assault incidents are indicated by warmer colors, showing where such crimes were most prevalent in the city.

```{r kd2}

as.data.frame(as_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
     geom_sf(aes(fill=value)) +
     geom_sf(data = sample_n(assault, 1500), size = .5) +
     scale_fill_viridis(name = "Density") +
     labs(title = "Kernel density of 2017 assault") +
     theme_void()
```

## Getting 2018 crime data

Let's see how our model performed relative to KD on the following year's data.

```{r 2018 data}
assault18 <- 
  read.socrata("https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy") %>% 
  filter(Primary.Type == "ASSAULT") %>%
  mutate(x = gsub("[()]", "", Location)) %>%
  separate(x,into= c("Y","X"), sep=",") %>%
  mutate(X = as.numeric(X),
         Y = as.numeric(Y)) %>% 
  na.omit %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102271') %>% 
  distinct() %>%
  .[fishnet,]
```

```{r new}

as_KDE_sum <- as.data.frame(as_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) 
kde_breaks <- classIntervals(as_KDE_sum$value, 
                             n = 5, "fisher")
as_KDE_sf <- as_KDE_sum %>%
  mutate(label = "Kernel Density",
         Risk_Category = classInt::findCols(kde_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(assault18) %>% mutate(asCount = 1), ., sum) %>%
    mutate(asCount = replace_na(asCount, 0))) %>%
  dplyr::select(label, Risk_Category, asCount)
```

```{r comp}
ml_breaks <- classIntervals(reg.ss.spatialCV$Prediction, 
                             n = 5, "fisher")
as_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category =classInt::findCols(ml_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(assault18) %>% mutate(asCount = 1), ., sum) %>%
      mutate(asCount = replace_na(asCount, 0))) %>%
  dplyr::select(label,Risk_Category, asCount)
```

We don't do quite as well because we don't have very many features, but still pretty good.

```{r comp plot}
rbind(as_KDE_sf, as_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(assault18, 3000), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2017 assault risk predictions; 2018 assault") +
    mapTheme(title_size = 14)
```
The maps show where assaults happened in 2017 and where they might happen in 2018. This shows us that we can use old data to guess where crimes might happen next.

```{r comp bar}
rbind(as_KDE_sf, as_risk_sf) %>%
  st_drop_geometry() %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countAssault = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Pcnt_of_test_set_crimes = countAssault / sum(countAssault)) %>%
    ggplot(aes(Risk_Category,Pcnt_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE, name = "Model") +
      labs(title = "Risk prediction vs. Kernel density, 2018 assaults",
           y = "% of Test Set Assaults (per model)",
           x = "Risk Category") +
  theme_bw() +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```
The bars show how closely the predictions match the real data, with differences indicating the accuracy of the risk prediction model. The maps and graphs indicate that the model can identify areas where assaults are more likely to occur, which implies that it has learned from the historical data effectively. By forecasting potential hotspots for criminal activity, the model could be a valuable tool for allocating police resources more efficiently, potentially preventing crime and increasing public safety.

The disparities between the kernel density and risk predictions raise questions about the model's precision. It's critical to ensure the model doesn't contribute to over-policing or bias against certain communities, which can happen if historical data reflects systemic inequalities or if the model hasn't been thoroughly validated against diverse datasets. Until the model is proven to be reliable, fair, and beneficial to the broader community, it's important to approach its full-scale implementation with a degree of caution.
